{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<h3>  Tools </h3>\n",
    "</center>\n",
    "<center>\n",
    "<h3> ToolKits</h3>\n",
    "</center>\n",
    "<center>\n",
    "<h3>Agents</h3>\n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "Tools are utilities desgined to be called by model : their inputs are desgined to be generated by the model and their output are desgigned to be passed back to the model.\n",
    "<i>Tools are needed when ever we want a model to control part of a code or call external API's</i>\n",
    "\n",
    "A Tool consist of \n",
    "<ul>\n",
    "<li>The <code>name</code> of the tool</li>\n",
    "<li>The <code>description</code> of what the tool does</li>\n",
    "<li>The <code>JSON schema </code> defining the input of the tool</li>\n",
    "<li>The <code>function</code> (and optionally, an async variant of the function)</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Two ways to call the tool</strong>\n",
    "<ol>\n",
    "<li>Invoke with just the arguments</li>\n",
    "<li>Invoke with tool call.</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Ways to create the tool\n",
    "<ol>\n",
    "<li>Using tool decorator</li>\n",
    "<li>Using the Agents</li>\n",
    "<li>By inheritaning <code>BaseTool</code> class</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "<p><code>Best Practices</code></p>\n",
    "When designing tools to be used by a model, it is important to keep in mind that :\n",
    "<ul>\n",
    "<li>Chat Model that have explicit <code>tool-calling API</code> will be better at tool calling then non fine tuned models.</li>\n",
    "<li>Model will perform better if the tools have well choosen names, descriptions ans JSON schemas. This is another form of prompt engineering</li>\n",
    "<li> Simply narrowly scoped tools are easier for models to use than complex tools</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToolKits\n",
    "<p>ToolKits are the collection of tools that are designed to be used together for a specific task. They have convenient loading methods</p>\n",
    "<p>All toolkits expose a <code>get_tools</code> method which return a list of tools.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 3px dotted red\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#initialize tool kit\\ntool_kit = ExampleToolkit(...)\\n#getting the list of tools\\ntools = tool_kit.get_tools()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#initialize tool kit\n",
    "tool_kit = ExampleToolkit(...)\n",
    "#getting the list of tools\n",
    "tools = tool_kit.get_tools()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "<p>By themselves language model can't take actions- they just ouput text</p>\n",
    "<p>Agents are system that use LLM as a reasoning engine to determine which action to take and what the input to those actions should be. The result of those action can be fed back into the agent and it determine whether more actions are needed or whether it is okay to finish.</p>\n",
    "\n",
    "<p><code>LangGraph</code> is used to create highly controllable and cutomizable agents</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <CODE>Different ways to implementing Tool </CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "# local model\n",
    "llm = Ollama(model=\"llama3.1\")  # To use it, run this in terminal. `ollama run llama3.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sample Tool using decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal call(sync)  6\n",
      "Async call  200\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool,ToolException\n",
    "@tool\n",
    "def multiply_tool_decorator(a:int,b:int) -> int:\n",
    "    \"\"\"_summary_\n",
    "    Multiply 2 numbers and return the value.Argumets are being type casted into int.\n",
    "\n",
    "    Args:\n",
    "        a (int): any number\n",
    "        b (int): any number\n",
    "\n",
    "    Returns:\n",
    "        int: any number\n",
    "    \"\"\"\n",
    "    if type(a) != int or type(b) != int:\n",
    "        raise ToolException(\"Both the arguemnts must be fo integer type\")\n",
    "    return a*b\n",
    "@tool\n",
    "async def amultiply_tool_decor(a:int,b:int )-> int:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        a (int): int\n",
    "        b (int): int\n",
    "\n",
    "    Returns:\n",
    "        int: int\n",
    "    \"\"\"\n",
    "    return int(a*b)\n",
    "# Tool initlization\n",
    "\n",
    "print(\"Normal call(sync) \",multiply_tool_decorator.invoke({\"a\":\"2\",\"b\":3}))\n",
    "print(\"Async call \", await amultiply_tool_decor.ainvoke({\"a\":10,\"b\":20}))\n",
    "# print(\"Normal call(sync) \",multiply_tool_decorator.invoke({\"a\":2,\"b\":\"3\"})) # No exception as being type casted in defined function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sample Tool by subclassing <code>BaseTool</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "from pydantic import BaseModel,Field # To create type\n",
    "from typing import Optional\n",
    "from langchain_core.callbacks import AsyncCallbackManagerForToolRun\n",
    "\n",
    "class CalculatorInput(BaseModel):\n",
    "    a:int = Field(description=\"First number\")\n",
    "    b: int = Field(description=\"Second number\")\n",
    "\n",
    "class CustomCalculatorTool(BaseTool):\n",
    "    name:str = \"Sample Calculator\"\n",
    "    description:str= \"For Simple mutliplication job\"\n",
    "    args_schema = CalculatorInput\n",
    "    return_direct: bool = True\n",
    "    \n",
    "    def _run(self,a:int,b:int, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> int :\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            a (int): int\n",
    "            b (int): int\n",
    "            run_manager (Optional[AsyncCallbackManagerForToolRun], optional): This Parameter is automatically\n",
    "            being passed by Langchain during the tool execution. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            int: _description_\n",
    "        \"\"\"\n",
    "        return a*b\n",
    "    \n",
    "    async def _arun(self,a:int,b:int, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> int:\n",
    "        return self._run(a,b,run_manager)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCalculatorTool()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply = CustomCalculatorTool()\n",
    "multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal call :  672\n",
      "Async call : 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal call : \",multiply.invoke({\"a\":12,\"b\":56}))\n",
    "print(\"Async call :\",await multiply.ainvoke({\"a\": 2, \"b\": 3}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
